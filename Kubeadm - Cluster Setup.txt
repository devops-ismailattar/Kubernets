
Update the system
$ sudo apt-get update
$ sudo apt-get install -y apt-transport-https ca-certificates curl gpg


Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL
$ sudo mkdir -p -m 755 /etc/apt/keyrings
$  curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

Add the appropriate Kubernetes apt repository
$ echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version:
$  sudo apt-get update
$  sudo apt-get install -y kubelet kubeadm kubectl
$  sudo apt-mark hold kubelet kubeadm kubectl


Enable the kubelet service before running kubeadm:
$ sudo systemctl enable --now kubelet

Additionally, after loading br_netfilter, it's a good practice to set some sysctl params required by Kubernetes:

$ sudo sysctl net.bridge.bridge-nf-call-iptables=1
$ sudo sysctl net.bridge.bridge-nf-call-ip6tables=1


$ sudo sysctl -w net.ipv4.ip_forward=1
$ echo "net.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.conf ## To make it permanent
$ sudo sysctl -p

install the container run time, as we stated earlier we will be using containerD as our container run time
$ sudo apt-get install -y containerd

Lets write the configuration file where containerD will expect to read from and enable the service
$ sudo mkdir -p /etc/containerd
$ sudo containerd config default | sudo tee /etc/containerd/config.toml
$ sudo systemctl restart containerd
$ sudo systemctl enable containerd


Now lets initialize the control plane and first control plane node

$ sudo kubeadm init --pod-network-cidr=192.168.0.0/16


Run below command- 

$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Now, let's install a network plugin to have the networking ready for the cluster; as we stated earlier, we will be using Calico as our CNI
$ kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.4/manifests/tigera-operator.yaml

Then we install custom resources for calico and the actual controllers
$ kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.4/manifests/custom-resources.yaml